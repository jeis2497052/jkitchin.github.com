---
title: Sensitivity analysis using automatic differentiation in Python
date: 2017/11/15 08:34:29
updated: 2017/11/15 08:34:29
categories: python, autograd, sensitivity
tags: 
---


<p>
This <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.428.6699&amp;rep=rep1&amp;type=pdf">paper</a> describes how sensitivity analysis requires access to the derivatives of a function. Say, for example we have a function describing the time evolution of the concentration of species A:
</p>

<p>
\[[A] = \frac{[A]_0}{k_1 + k_{-1}} (k_1 \exp(-(k_1 _ k_{-1})t) + k_{-1})\]
</p>

<p>
The local sensitivity of the concentration of A to the parameters \(k1\) and \(k_1\) are defined as \(\frac{\partial A}{\partial k1}\) and \(\frac{\partial A}{\partial k_1}\). Our goal is to plot the sensitivity as a function of time. We could derive those derivatives, but we will use auto-differentiation instead through the autograd package. Here we import numpy from the autograd package and plot the function above.
</p>

<div class="org-src-container">
<pre class="src src-ipython" id="orgca5f78d"><span style="color: #0000FF;">import</span> autograd.numpy <span style="color: #0000FF;">as</span> np

<span style="color: #BA36A5;">A0</span> = 1.0

<span style="color: #0000FF;">def</span> <span style="color: #006699;">A</span>(t, k1, k_1):
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #0000FF;">return</span> A0 / (k1 + k_1) * (k1 * np.exp(-(k1 + k_1) * t) + k_1)

%matplotlib inline
<span style="color: #0000FF;">import</span> matplotlib.pyplot <span style="color: #0000FF;">as</span> plt

<span style="color: #BA36A5;">t</span> = np.linspace(0, 0.5)

<span style="color: #BA36A5;">k1</span> = 3.0
<span style="color: #BA36A5;">k_1</span> = 3.0
plt.plot(t, A(t, k1, k_1))
plt.xlim([0, 0.5])
plt.ylim([0, 1])
plt.xlabel(<span style="color: #008000;">'t'</span>)
plt.ylabel(<span style="color: #008000;">'A'</span>)
</pre>
</div>

<p>
<img src="/media/ob-ipython-09dd39779fdcdb6e3f00397800ec05e6.png"> 
</p>

<p>
The figure above reproduces Fig. 1 from the paper referenced above.  Next, we use autograd to get the derivatives. This is subtly different than our previous <a href="http://kitchingroup.cheme.cmu.edu/blog/2017/11/14/Forces-by-automatic-differentiation-in-molecular-simulation/">post</a>. First, we need the derivative of the function with respect to the second and third arguments; the default is the first argument. Second, we want to evaluate this derivative at each time value. We use the jacobian function in autograd to get these. This is different than grad, which will sum up the derivatives at each time. That might be useful for regression, but not for sensitivity analysis. Finally, to reproduce Figure 2a, we plot the absolute value of the sensitivities.
</p>

<div class="org-src-container">
<pre class="src src-ipython" id="org06c1b64"><span style="color: #0000FF;">from</span> autograd <span style="color: #0000FF;">import</span> jacobian

<span style="color: #BA36A5;">dAdk1</span> = jacobian(A, 1)
<span style="color: #BA36A5;">dAdk_1</span> = jacobian(A, 2)

plt.plot(t, np.<span style="color: #006FE0;">abs</span>(dAdk1(t, k1, k_1)))
plt.plot(t, np.<span style="color: #006FE0;">abs</span>(dAdk_1(t, k1, k_1)))
plt.xlim([0, 0.5])
plt.ylim([0, 0.1])
plt.xlabel(<span style="color: #008000;">'t'</span>)
plt.legend([<span style="color: #008000;">'$S_{k1}$'</span>, <span style="color: #008000;">'$S_{k\_1}$'</span>])
</pre>
</div>

<p>
<img src="/media/ob-ipython-f3534f038e5e3a7c77041501838e9fdb.png"> 
</p>

<p>
That looks like the figure in the paper. To summarize the main takeaway, autograd enabled us to readily compute derivatives without having to derive them manually. There was a little subtlety in choosing jacobian over grad or elementwise_grad but once you know what these do, it seems reasonable. It is important to import the wrapped numpy first, to enable autograd to do its work. All the functions here are pretty standard, so everything worked out of the box. We should probably be using autograd, or something like it for more things in science!
</p>
<p>Copyright (C) 2017 by John Kitchin. See the <a href="/copying.html">License</a> for information about copying.<p>
<p><a href="/org/2017/11/15/Sensitivity-analysis-using-automatic-differentiation-in-Python.org">org-mode source</a></p>
<p>Org-mode version = 9.1.2</p>