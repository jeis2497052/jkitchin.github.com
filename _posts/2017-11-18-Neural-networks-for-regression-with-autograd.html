---
title: Neural networks for regression with autograd
date: 2017/11/18 14:20:17
updated: 2017/11/18 14:20:17
categories: python, autograd
tags: 
---


<p>
Today we are going to take a meandering path to using autograd to train a neural network for regression. First let's consider this very general looking nonlinear model that we might fit to data. There are 10 parameters in it, so we should expect we can get it to fit some data pretty well. 
</p>

<p>
\(y = b1 + w10 tanh(w00 x + b00) + w11 tanh(w01 x + b01) + w12 tanh(w02 x + b02)\)
</p>

<p>
We will use it to fit data that is generated from \(y = x^\frac{1}{3}\). First, we just do a least_squares fit. This function can take a jacobian function, so we provide one using autograd. 
</p>

<div class="org-src-container">
<pre class="src src-ipython" id="orge5f5d5d"><span style="color: #0000FF;">import</span> autograd.numpy <span style="color: #0000FF;">as</span> np
<span style="color: #0000FF;">from</span> autograd <span style="color: #0000FF;">import</span> jacobian

<span style="color: #0000FF;">from</span> scipy.optimize <span style="color: #0000FF;">import</span> curve_fit

<span style="color: #8D8D84;"># </span><span style="color: #8D8D84; font-style: italic;">Some generated data</span>
<span style="color: #BA36A5;">X</span> = np.linspace(0, 1)
<span style="color: #BA36A5;">Y</span> = X**(1. / 3.)

<span style="color: #0000FF;">def</span> <span style="color: #006699;">model</span>(x, *pars):
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #BA36A5;">b1</span>, <span style="color: #BA36A5;">w10</span>, <span style="color: #BA36A5;">w00</span>, <span style="color: #BA36A5;">b00</span>, <span style="color: #BA36A5;">w11</span>, <span style="color: #BA36A5;">w01</span>, <span style="color: #BA36A5;">b01</span>, <span style="color: #BA36A5;">w12</span>, <span style="color: #BA36A5;">w02</span>, <span style="color: #BA36A5;">b02</span> = pars
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #BA36A5;">pred</span> = b1 + w10 * np.tanh(w00 * x + b00) + w11 * np.tanh(w01 * x + b01) + w12 * np.tanh(w02 * x + b02)
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #0000FF;">return</span> pred


<span style="color: #0000FF;">def</span> <span style="color: #006699;">resid</span>(pars):
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #0000FF;">return</span> Y - model(X, *pars)
</pre>
</div>

<pre class="example">
MSE:  0.0744600049689

</pre>

<p>
We will look at some timing of this regression. Here we do not provide a jacobian.
</p>

<div class="org-src-container">
<pre class="src src-ipython" id="orgeed054e">%%timeit
<span style="color: #BA36A5;">pars</span> = least_squares(resid, np.random.randn(10)*0.1).x
</pre>
</div>

<pre class="example">
1.21 s ± 42.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

</pre>

<p>
And here we do provide one. It takes <i>a lot</i> longer to do this. We do have a jacobian of 10 parameters, so that ends up being a lot of extra computations to do.
</p>

<div class="org-src-container">
<pre class="src src-ipython" id="org6e96774">%%timeit
<span style="color: #BA36A5;">pars</span> = least_squares(resid, np.random.randn(10)*0.1, jac=jacobian(resid)).x
</pre>
</div>

<pre class="example">
24.1 s ± 1.61 s per loop (mean ± std. dev. of 7 runs, 1 loop each)

</pre>

<p>
We will print these parameters for reference later.
</p>

<div class="org-src-container">
<pre class="src src-ipython" id="org98d5e21"><span style="color: #BA36A5;">b1</span>, <span style="color: #BA36A5;">w10</span>, <span style="color: #BA36A5;">w00</span>, <span style="color: #BA36A5;">b00</span>, <span style="color: #BA36A5;">w11</span>, <span style="color: #BA36A5;">w01</span>, <span style="color: #BA36A5;">b01</span>, <span style="color: #BA36A5;">w12</span>, <span style="color: #BA36A5;">w02</span>, <span style="color: #BA36A5;">b02</span> = pars

<span style="color: #0000FF;">print</span>([w00, w01, w02], [b00, b01, b02])
<span style="color: #0000FF;">print</span>([w10, w11, w12], b1)
</pre>
</div>

<pre class="example">
[5.3312122926210703, 54.6923797622945, -0.50881373227993232] [2.9834159679095662, 2.6062295455987199, -2.3782572250527778]
[42.377172168160477, 22.036104340171004, -50.075636975961089] -113.179935862

</pre>

<p>
Let's just make sure the fit looks ok. I am going to plot it outside the fitted region to see how it extrapolates. The shaded area shows the region we did the fitting in.
</p>

<div class="org-src-container">
<pre class="src src-ipython" id="org50d6e73"><span style="color: #BA36A5;">X2</span> = np.linspace(0, 3)
<span style="color: #BA36A5;">Y2</span> = X2**(1. / 3.)

<span style="color: #BA36A5;">Z2</span> = model(X2, *pars)

plt.plot(X2, Y2, <span style="color: #008000;">'b.'</span>, label=<span style="color: #008000;">'analytical'</span>)
plt.plot(X2, Z2, label=<span style="color: #008000;">'model'</span>)
plt.fill_between(X2 &lt; 1, 0, 1.4, facecolor=<span style="color: #008000;">'gray'</span>, alpha=0.5)
</pre>
</div>

<p>
<img src="/media/ob-ipython-1cd291f9bced2fb821bee79f39a275cc.png"> 
</p>

<p>
You can seen it fits pretty well from 0 to 1 where we fitted it, but outside that the model is not accurate. Our model is not that related to the true function of the model, so there is no reason to expect it should extrapolate.
</p>

<p>
I didn't pull that model out of nowhere. Let's rewrite it in a few steps. If we think of tanh as a function that operates element-wise on a vector, we could write that equation more compactly at:
</p>

<pre class="example">
                              [w00 * x + b01] 
y = [w10, w11, w12] @ np.tanh([w01 * x + b01]) + b1
                              [w02 * x + b02]  
</pre>

<p>
We can rewrite this one more time in matrix notation:
</p>

<pre class="example">
y = w1 @ np.tanh(w0 @ x + b0) + b1
</pre>

<p>
Another way to read these equations is that we have an input of x. We multiply the input by a vector weights (w0), add a vector of offsets (biases), b0, activate that by the nonlinear tanh function, then multiply that by a new set of weights, and add a final bias. We typically call this kind of model a neural network. There is an input layer, one hidden layer with 3 neurons that are activated by tanh, and one output layer with linear activation.
</p>

<p>
Autograd was designed in part for building neural networks. In the next part of this post, we reformulate this regression as a neural network. This code is lightly adapted from <a href="https://github.com/HIPS/autograd/blob/master/examples/neural_net_regression.py">https://github.com/HIPS/autograd/blob/master/examples/neural_net_regression.py</a>.
</p>

<p>
The first function initializes the weights and biases for each layer in our network. It is standard practice to initialize them to small random numbers to avoid any unintentional symmetries that might occur from a systematic initialization (e.g. all ones or zeros). The second function sets up the neural network and computes its output. 
</p>

<div class="org-src-container">
<pre class="src src-ipython" id="orgdc568cb"><span style="color: #0000FF;">from</span> autograd <span style="color: #0000FF;">import</span> grad
<span style="color: #0000FF;">import</span> autograd.numpy.random <span style="color: #0000FF;">as</span> npr
<span style="color: #0000FF;">from</span> autograd.misc.optimizers <span style="color: #0000FF;">import</span> adam

<span style="color: #0000FF;">def</span> <span style="color: #006699;">init_random_params</span>(scale, layer_sizes, rs=npr.RandomState(0)):
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #036A07;">"""Build a list of (weights, biases) tuples, one for each layer."""</span>
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #0000FF;">return</span> [(rs.randn(insize, outsize) * scale,   <span style="color: #8D8D84;"># </span><span style="color: #8D8D84; font-style: italic;">weight matrix</span>
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>rs.randn(outsize) * scale)           <span style="color: #8D8D84;"># </span><span style="color: #8D8D84; font-style: italic;">bias vector</span>
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #0000FF;">for</span> insize, outsize <span style="color: #0000FF;">in</span> <span style="color: #006FE0;">zip</span>(layer_sizes[:-1], layer_sizes[1:])]

<span style="color: #0000FF;">def</span> <span style="color: #006699;">nn_predict</span>(params, inputs, activation=np.tanh):
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #0000FF;">for</span> W, b <span style="color: #0000FF;">in</span> params[:-1]:
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #BA36A5;">outputs</span> = np.dot(inputs, W) + b
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #BA36A5;">inputs</span> = activation(outputs)
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #8D8D84;"># </span><span style="color: #8D8D84; font-style: italic;">no activation on the last layer</span>
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #BA36A5;">W</span>, <span style="color: #BA36A5;">b</span> = params[-1]
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #0000FF;">return</span> np.dot(inputs, W) + b
</pre>
</div>

<p>
Here we use the first function to define the weights and biases for a neural network with one input, one hidden layer of 3 neurons, and one output layer. 
</p>

<div class="org-src-container">
<pre class="src src-ipython" id="org32ea84a"><span style="color: #BA36A5;">init_scale</span> = 0.1
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   
<span style="color: #8D8D84;"># </span><span style="color: #8D8D84; font-style: italic;">Here is our initial guess:</span>
<span style="color: #BA36A5;">params</span> = init_random_params(init_scale, layer_sizes=[1, 3, 1])
<span style="color: #0000FF;">for</span> i, wb <span style="color: #0000FF;">in</span> <span style="color: #006FE0;">enumerate</span>(params):
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #BA36A5;">W</span>, <span style="color: #BA36A5;">b</span> = wb
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #0000FF;">print</span>(<span style="color: #008000;">'w{0}: {1}, b{0}: {2}'</span>.<span style="color: #006FE0;">format</span>(i, W.shape, b.shape))
</pre>
</div>

<pre class="example">
w0: (1, 3), b0: (3,)
w1: (3, 1), b1: (1,)

</pre>

<p>
You can see w0 is a column vector of weights, and there are three biases in b0. W1 in contrast, is a row vector of weights, with one bias. So 10 parameters in total, like we had before. We will create an objective function of the mean squared error again, and a callback function to show us the progress.
</p>

<p>
Then we run the optimization step iteratively until we get our objective function below a tolerance we define.
</p>

<div class="org-src-container">
<pre class="src src-ipython" id="org1ff81af"><span style="color: #0000FF;">def</span> <span style="color: #006699;">objective</span>(params, _):
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #BA36A5;">pred</span> = nn_predict(params, X.reshape([-1, 1]))
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #BA36A5;">err</span> = Y.reshape([-1, 1]) - pred
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #0000FF;">return</span> np.mean(err**2)


<span style="color: #0000FF;">def</span> <span style="color: #006699;">callback</span>(params, step, g):
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #0000FF;">if</span> step % 250 == 0:
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #0000FF;">print</span>(<span style="color: #008000;">"Iteration {0:3d} objective {1:1.2e}"</span>.<span style="color: #006FE0;">format</span>(i * N + step,
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>  objective(params, step)))

<span style="color: #BA36A5;">N</span> = 500
<span style="color: #BA36A5;">NMAX</span> = 20

<span style="color: #0000FF;">for</span> i <span style="color: #0000FF;">in</span> <span style="color: #006FE0;">range</span>(NMAX):
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #BA36A5;">params</span> = adam(grad(objective), params,
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span> step_size=0.01, num_iters=N, callback=callback)
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #0000FF;">if</span> objective(params, _) &lt; 2e-5:
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #0000FF;">break</span>
</pre>
</div>

<pre class="example">
Iteration   0 objective 5.30e-01
Iteration 250 objective 4.52e-03
Iteration 500 objective 4.17e-03
Iteration 750 objective 1.86e-03
Iteration 1000 objective 1.63e-03
Iteration 1250 objective 1.02e-03
Iteration 1500 objective 6.30e-04
Iteration 1750 objective 4.54e-04
Iteration 2000 objective 3.25e-04
Iteration 2250 objective 2.34e-04
Iteration 2500 objective 1.77e-04
Iteration 2750 objective 1.35e-04
Iteration 3000 objective 1.04e-04
Iteration 3250 objective 7.86e-05
Iteration 3500 objective 5.83e-05
Iteration 3750 objective 4.46e-05
Iteration 4000 objective 3.39e-05
Iteration 4250 objective 2.66e-05
Iteration 4500 objective 2.11e-05
Iteration 4750 objective 1.71e-05

</pre>


<p>
Let's compare these parameters to the previous ones we got.
</p>

<div class="org-src-container">
<pre class="src src-ipython" id="org3b106f1"><span style="color: #0000FF;">for</span> i, wb <span style="color: #0000FF;">in</span> <span style="color: #006FE0;">enumerate</span>(params):
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #BA36A5;">W</span>, <span style="color: #BA36A5;">b</span> = wb
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #0000FF;">print</span>(<span style="color: #008000;">'w{0}: {1}, b{0}: {2}'</span>.<span style="color: #006FE0;">format</span>(i, W, b))
</pre>
</div>

<pre class="example">
w0: [[ -0.71332351   3.23209728 -32.51135373]], b0: [ 0.45819205  0.19314303 -0.8687    ]
w1: [[-0.53699549]
 [ 0.39522207]
 [-1.05457035]], b1: [-0.58005452]

</pre>

<p>
These look pretty different. It is not too surprising that there could be more than one set of these parameters that give similar fits. The original data only requires two parameters to create it: \(y = a x^b\), where \(x=1\) and \(b=1/3\). We have 8 extra parameters of flexibility in this model.
</p>

<p>
Let's again examine the fit of our model to the data. 
</p>

<div class="org-src-container">
<pre class="src src-ipython" id="org36ec841"><span style="color: #BA36A5;">Z2</span> = nn_predict(params, X2.reshape([-1, 1]))

plt.plot(X2, Y2, <span style="color: #008000;">'b.'</span>, label=<span style="color: #008000;">'analytical'</span>)
plt.plot(X2, Z2, label=<span style="color: #008000;">'NN'</span>)
plt.fill_between(X2 &lt; 1, 0, 1.4, facecolor=<span style="color: #008000;">'gray'</span>, alpha=0.5)
</pre>
</div>

<p>
<img src="/media/ob-ipython-d6093b26f8e4680ebc8d7b2ebbcbf56b.png"> 
</p>


<p>
Once again, we can see that between 0 and 1 where the model was fitted we get a good fit, but past that the model does not fit the known function well. It is coincidentally better than our previous model, but as before it is not advisable to use this model for extrapolation. Even though we say it "learned" something about the data, it clearly did not learn the function \(y=x^{1/3}\). It did "learn" some approximation to it in the region of x=0 to 1. Of course, it did not learn anything that the first nonlinear regression model didn't learn. 
</p>

<p>
Now you know the secret of a neural network, it is just a nonlinear model. Without the activation, it is just a linear model. So, why use linear regression, when you can use an unactivated neural network and call it AI?
</p>
<p>Copyright (C) 2017 by John Kitchin. See the <a href="/copying.html">License</a> for information about copying.<p>
<p><a href="/org/2017/11/18/Neural-networks-for-regression-with-autograd.org">org-mode source</a></p>
<p>Org-mode version = 9.1.2</p>