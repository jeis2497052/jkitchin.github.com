* Timing Lennard-Jones implementations - ASE vs autograd
  :PROPERTIES:
  :categories: python, autograd, lennardjones
  :date:     2017/11/20 21:19:17
  :updated:  2017/11/20 21:19:17
  :org-url:  http://kitchingroup.cheme.cmu.edu/org/2017/11/20/Timing-Lennard-Jones-implementations---ASE-vs-autograd.org
  :permalink: http://kitchingroup.cheme.cmu.edu/blog/2017/11/20/Timing-Lennard-Jones-implementations---ASE-vs-autograd/index.html
  :END:

In a comment on this [[http://kitchingroup.cheme.cmu.edu/blog/2017/11/14/Forces-by-automatic-differentiation-in-molecular-simulation/][post]] Konrad Hinsen asked if the autograd forces on a Lennard-Jones potential would be useable in production. I wasn't sure, and was suspicious that analytical force functions would be faster. It turns out to not be so simple. In this post, I attempt to do some timing experiments for comparison. These are tricky to do right, and in a meaningful way, so I will start by explaining what is tricky and why I think the results are meaningful. 

The ASE calculators cache their results, and return the cached results after the first run. We will do these on a 13-atom icosahedron cluster.

#+NAME: paris-single-michigan-london
#+BEGIN_SRC ipython
from ase.calculators.lj import LennardJones
from ase.cluster.icosahedron import Icosahedron

atoms = Icosahedron('Ar', noshells=2, latticeconstant=3)
atoms.set_calculator(LennardJones())

import time
t0 = time.time()
print(atoms.get_potential_energy())
print(time.time() - t0)

t0 = time.time()
print(atoms.get_potential_energy())
print(time.time() - t0)

atoms.calc.results = {}
t0 = time.time()
print(atoms.get_potential_energy())
print(time.time() - t0)
#+END_SRC

#+RESULTS: paris-single-michigan-london
:RESULTS:
#+BEGIN_EXAMPLE
-1.25741774649
0.004888057708740234
-1.25741774649
0.0008807182312011719
-1.25741774649
0.0034499168395996094

#+END_EXAMPLE
:END:

Note the approximate order of magnitude reduction in elapsed time for the second call. After that, we reset the calculator results, and the time goes back up. So, we have to incorporate that into our timing. Also, in the ASE calculator, the forces are simultaneously calculated. There isn't a way to separate the calls. I am going to use the timeit feature in Ipython for the timing. I don't have a lot of control over what else is running on my machine, so I have observed some variability in the timing results. Finally, I am running these on a MacBook Air.

#+NAME: early-six-five-burger
#+BEGIN_SRC ipython
%%timeit
atoms.get_potential_energy()
atoms.calc.results = {} # this resets it so it runs each time. Otherwise, we get cached results
#+END_SRC

#+RESULTS: early-six-five-burger
:RESULTS:
#+BEGIN_EXAMPLE
1.25 ms ± 34.8 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)

#+END_EXAMPLE
:END:

That seems like a surprisingly long time. If you neglect the calculator reset, it looks about 10 times faster because the cache lookup is fast!

Let's compare that to an implementation of the Lennard-Jones potential similar to the last time. This implementation differs from [[http://kitchingroup.cheme.cmu.edu/blog/2017/11/14/Forces-by-automatic-differentiation-in-molecular-simulation/][the first one I blogged about]]. This one is fully vectorized. It still does not support periodic boundary conditions though. This version may be up to 10 times faster than the previous version. I haven't tested this very well, I only assured it gives the same energy as ASE for the example in this post.

#+NAME: ten-neptune-oregon-sodium
#+BEGIN_SRC ipython
import autograd.numpy as np

def energy(positions):
    "Compute the energy of a Lennard-Jones system."
    
    sigma = 1.0
    epsilon = 1.0
    rc = 3 * sigma

    e0 = 4 * epsilon * ((sigma / rc)**12 - (sigma / rc)**6)
    
    natoms = len(positions)
    # These are the pairs of indices we need to compute distances for.
    a, b = np.triu_indices(natoms, 1)

    d = positions[a] - positions[b]
    r2 = np.sum(d**2, axis=1)
    c6 = np.where(r2 <= rc**2, (sigma**2 / r2)**3, np.zeros_like(r2))
    
    energy = -e0 * (c6 != 0.0).sum()
    c12 = c6**2
    energy += np.sum(4 * epsilon * (c12 - c6))
    
    return energy

print(energy(atoms.positions))
#+END_SRC

#+RESULTS: ten-neptune-oregon-sodium
:RESULTS:
#+BEGIN_EXAMPLE
-1.25741774649

#+END_EXAMPLE
:END:

We store the positions in a variable, in case there is any lookup time, since this function only needs an array.

#+NAME: leopard-texas-lemon-minnesota
#+BEGIN_SRC ipython
pos = atoms.positions
#+END_SRC

There is no caching to worry about here, we can just get the timing.

#+NAME: hotel-hamper-hamper-march
#+BEGIN_SRC ipython
%%timeit
energy(pos)
#+END_SRC

#+RESULTS: hotel-hamper-hamper-march
:RESULTS:
#+BEGIN_EXAMPLE
82.5 µs ± 4.55 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)

#+END_EXAMPLE
:END:

Wow, that is a lot faster than the ASE implementation. Score one for vectorization.

#+NAME: cola-alaska-cold-bakerloo
#+BEGIN_SRC ipython
print('Vectorized is {0:1.1f} times faster than ASE at energy.'.format(1.25e-3 / 82.5e-6))
#+END_SRC

#+RESULTS: cola-alaska-cold-bakerloo
:RESULTS:
#+BEGIN_EXAMPLE
Vectorized is 15.2 times faster than ASE at energy.

#+END_EXAMPLE
:END:

Yep, a fully vectorized implementation is a lot faster than the ASE version which uses loops. So far the difference has nothing to do with autograd.

** Timing on the forces
   
The forces are where derivatives are important, and it is a reasonable question of whether hand-coded derivatives are faster or slower than autograd derivatives. We first look at the forces from ASE. The analytical forces take about the same time as the energy, which is not surprising. The same work is done for both of them.

#+NAME: wolfram-march-fourteen-jersey
#+BEGIN_SRC ipython
%%timeit
atoms.get_forces()
atoms.calc.results = {}
#+END_SRC

#+RESULTS: wolfram-march-fourteen-jersey
:RESULTS:
#+BEGIN_EXAMPLE
1.21 ms ± 16 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)

#+END_EXAMPLE
:END:

Here is our auto-differentiated force function.

#+NAME: diet-zulu-artist-red
#+BEGIN_SRC ipython
from autograd import elementwise_grad

def forces(pos):
    dEdR = elementwise_grad(energy)
    return -dEdR(pos)
#+END_SRC

And now performance for that:

#+NAME: fillet-floor-oregon-south
#+BEGIN_SRC ipython
%%timeit 

forces(pos)
#+END_SRC

#+RESULTS: fillet-floor-oregon-south
:RESULTS:
#+BEGIN_EXAMPLE
335 µs ± 15.9 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)

#+END_EXAMPLE
:END:

This is faster than the ASE version. I suspect that it is largely because of the faster, vectorized algorithm overall. 

#+NAME: whiskey-eighteen-missouri-seven
#+BEGIN_SRC ipython
print('autograd is {0:1.1f} times faster than ASE on forces.'.format(1.21e-3 / 335e-6))
#+END_SRC

#+RESULTS: whiskey-eighteen-missouri-seven
:RESULTS:
#+BEGIN_EXAMPLE
autograd is 3.6 times faster than ASE on forces.

#+END_EXAMPLE
:END:

The forces are about about four times slower than the energy time. It could be possible to hand-code a faster function for the forces, if it was fully vectorized. I spent a while seeing what would be required for that, and it is not obvious how to do that. Any solution that uses loops will be slower I think.

This doesn't directly answer the question of whether this can work in production. Everything is still written in Python here, which might limit the size and length of calculations you can practically do. With the right implementation though, it looks promising.



